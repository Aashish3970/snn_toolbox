{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Model Specific Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loihi N2 contraint, configurable in N3 (on a core by core basis)\n",
    "numCompartmentsPerCore = 1024\n",
    "numCoresPerChip = 128\n",
    "\n",
    "# Reduces the number of channels in each layer by a fraction\n",
    "thinning = 1\n",
    "\n",
    "# Reduces the xy dimension of each layer by a fraction\n",
    "resolutionMultiplier = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Yolo v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tinyYoloV3():\n",
    "    \"\"\"Define the Tiny Yolo v3 model\n",
    "    Considering only the convolution layers (how would max pooling map to hardware?)\n",
    "    \"\"\"\n",
    "    \n",
    "    # how many filters in each layer\n",
    "    allLayers = dict(numFilters = [16, 32, 64, 128, 256, 512, 1024, 256, 512, 255, 128, 256, 255],\n",
    "                    filter_x = [3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 3, 1], # The filter x-y size and stride\n",
    "                    filter_y = [3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 3, 1],\n",
    "                    stride = [1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "                    input_w = [416, 208, 104, 52, 26, 13, 13, 13, 13, 13, 13, 26, 26],    # Input dimensions for each channel\n",
    "                    input_h = [416, 208, 104, 52, 26, 13, 13, 13, 13, 13, 13, 26, 26],\n",
    "                    input_c = [3, 16, 32, 64, 128, 256, 512, 1024, 256, 512, 256, 384, 256],\n",
    "                    numGroups = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], # Groups and depthwise convolution are not part of Yolo model\n",
    "                    depthWise = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "    return allLayers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobileNetV2():\n",
    "    \"\"\"Define the MobileNet v2 model\n",
    "    RESTRICTIONS???\n",
    "    \"\"\"\n",
    "        \n",
    "    def appendLayer(layer1, layer2):\n",
    "        \n",
    "        if layer1.keys():\n",
    "            combined = dict()\n",
    "            for kk in layer1.keys():\n",
    "                combined.update({kk: layer1[kk] + layer2[kk]})\n",
    "        else:\n",
    "            combined = layer2\n",
    "            \n",
    "        return combined\n",
    "    \n",
    "    \n",
    "    def mobileNetLayerGroup(t, cIn, cOut, n, s, w, h):\n",
    "        \"\"\"Create a group of mobileNet v2 layers as per https://arxiv.org/pdf/1801.04381.pdf\n",
    "        \"\"\"\n",
    "        def mobileNetLayer(t, cIn, cOut, s, w, h):\n",
    "            \"\"\"Create a single mobileNet v2 layer, which consists of multiple intenal layers\"\"\"\n",
    "            layerDict = dict(numFilters=[cIn*t, cIn*t, cOut], \n",
    "                             filter_x=[1,3,1],\n",
    "                             filter_y=[1,3,1],\n",
    "                             stride=[1,s,1],\n",
    "                             input_w=[w,w,w/s],\n",
    "                             input_h=[h,h,h/s],\n",
    "                             input_c=[cIn, cIn*t, cIn*t],\n",
    "                             numGroups=[1,1,1],\n",
    "                             depthWise=[0,1,0])\n",
    "            return layerDict\n",
    "\n",
    "        \n",
    "        # The first block in a group implements the stride\n",
    "        layerDict = mobileNetLayer(t, cIn, cOut, s, w, h)\n",
    "        groupDict = layerDict\n",
    "\n",
    "        # The remaining blocks are identical and have stride 1\n",
    "        layerDict = mobileNetLayer(t, cOut, cOut, s, w/s, h/s)\n",
    "        for nn in range(1,n):\n",
    "            groupDict = appendLayer(groupDict, layerDict)\n",
    "                \n",
    "        return groupDict\n",
    "    \n",
    "    # from https://arxiv.org/pdf/1801.04381.pdf Table 2\n",
    "    t = [1, 6, 6, 6, 6, 6, 6]\n",
    "    c = [32, 16, 24, 32, 64, 96, 160, 320]\n",
    "    cIn  = c[:-1]\n",
    "    cOut = c[1:]\n",
    "    n = [1, 2, 3, 4, 3, 3, 1]\n",
    "    s = [1, 2, 2, 2, 1, 2, 1]\n",
    "    w = [112, 112, 56, 28, 14, 14, 7]\n",
    "    h = w\n",
    "    \n",
    "    # First layer is full convolution\n",
    "    allLayers = dict(numFilters=[32],\n",
    "                   filter_x=[3],\n",
    "                   filter_y=[3],\n",
    "                   stride=[2],\n",
    "                   input_w=[224],\n",
    "                   input_h=[224],\n",
    "                   input_c=[3],\n",
    "                   numGroups=[1],\n",
    "                   depthWise=[0])\n",
    "    \n",
    "    # next layers are repeated building blocks\n",
    "    for gg in range(len(t)):\n",
    "        groupLayers = mobileNetLayerGroup(t[gg], cIn[gg], cOut[gg], n[gg], s[gg], w[gg], h[gg])\n",
    "        allLayers = appendLayer(allLayers, groupLayers) \n",
    "        \n",
    "    lastLayers = dict(numFilters=[1280],\n",
    "                      filter_x=[1],\n",
    "                      filter_y=[1],\n",
    "                      stride=[1],\n",
    "                      input_w=[7],\n",
    "                      input_h=[7],\n",
    "                      input_c=[320],\n",
    "                      numGroups=[1],\n",
    "                      depthWise=[0])\n",
    "    \n",
    "    allLayers = appendLayer(allLayers, lastLayers) \n",
    "    \n",
    "\n",
    "    return allLayers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose which model to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allLayers = mobileNetV2()\n",
    "#allLayers = tinyYoloV3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFilters = np.array(allLayers[\"numFilters\"])\n",
    "filter_x   = np.array(allLayers[\"filter_x\"])\n",
    "filter_y   = np.array(allLayers[\"filter_y\"])\n",
    "stride     = np.array(allLayers[\"stride\"])\n",
    "input_w    = np.array(allLayers[\"input_w\"])\n",
    "input_h    = np.array(allLayers[\"input_h\"])\n",
    "input_c    = np.array(allLayers[\"input_c\"])\n",
    "numGroups  = np.array(allLayers[\"numGroups\"])\n",
    "depthWise  = np.array(allLayers[\"depthWise\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output parameters\n",
    "\n",
    "Parameters without taking into account the core structure of Loihi (i.e. assumes Loihi system is one big core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numSplits is implementation specific, not model specific\n",
    "# Most likely numSplits would be determined automatically by a mapper\n",
    "numSplits = np.ones(shape=(numFilters.shape))\n",
    "\n",
    "# determine how thinning and reducing resolution affects the model\n",
    "numFilters = numFilters*thinning\n",
    "input_c = input_c*thinning\n",
    "input_h = input_h*resolutionMultiplier\n",
    "input_w = input_w*resolutionMultiplier\n",
    "\n",
    "\n",
    "# channels per filter\n",
    "filter_c = input_c/numGroups\n",
    "filter_c[depthWise==1] = 1\n",
    "\n",
    "# How many input synapses per neuron\n",
    "fanIn = filter_x*filter_y*filter_c\n",
    "\n",
    "# How many neurons (synapses) does each input spike go to\n",
    "fanOut = (filter_x/stride)*(filter_y/stride)*numFilters*filter_c\n",
    "\n",
    "numNeurons = (input_w/stride)*(input_h/stride)*numFilters\n",
    "\n",
    "# Each SynOp is 1 MAC, which is 2 FLOPs in darknet (1 mult + 1 add)\n",
    "numSynapses = numNeurons*fanIn\n",
    "\n",
    "numWeights = numFilters*fanIn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per Core\n",
    "\n",
    "Thus far parameters indicate \"averages\" without quantizing to core boundaries. Actual numbers can be quite different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groups is a form of core splitting where each input only fans out to numFilters/numGroups different filters\n",
    "# The weight memory is reduced to a fraction (1/numGroups)^2 because the number of filters per core reduces\n",
    "# by 1/numGroups, and because each filter size reduces by 1/numGroups\n",
    "# i.e. for 2 groups (numGroups=2), the \"numFilters\" different filters at a given x-y location could be split \n",
    "# across 2 cores, each core holding numFilters/numGroups different filters.  Each input would only need to be \n",
    "# routed to one of these cores. Weight memory gets reduced by 1/2*1/2 because number of filters per core halves\n",
    "# and each individual filter sizes halves\n",
    "#\n",
    "# Core splitting does something similar, (splits numFilters across \"numCoreSplits\" cores).\n",
    "# The number of filters on a single core is now numFilters/numCoreSplits, so weight memory also reduces by\n",
    "# 1/numCoreSplits.\n",
    "# Each filter is still full sized so fan-in fan-out remain the same.\n",
    "# Each input spike will need to be routed to numCoreSplits different cores to hit all filters (plus additional \n",
    "# cores if its projection crosses core boundaries)\n",
    "numCoreSplits = np.ones(shape=numFilters.shape)\n",
    "\n",
    "# Assuming numFilters/numGroups/numCoreSplits different filters on a single core\n",
    "# groups can be used to reduce memory/computation requirements at the potential cost of accuracy\n",
    "# numCoreSplits can be used to split computation across cores (reduce per-core weight memory) \n",
    "# at no accuracy cost, but require more spike routing (each core need to receive the input spikes)\n",
    "# splitting across cores allows each core to process more x-y region, which also reduces the need\n",
    "# for routing multiple spikes at core edges, so there is a tradeoff\n",
    "filtersPerCore = numFilters/numGroups/numCoreSplits\n",
    "\n",
    "# filter extent in x and y assuming square filters. A number less than 1 cannot be accommodated. A number of 1\n",
    "# means that only 1 x-y location is processed by the core, so if filters are of size x*y, each input spikes will\n",
    "# need to project to x*y different cores to cover all locations (plus additional cores due to numCoreSplits)\n",
    "filterExtentPerCore_x = np.sqrt(np.divide(numCompartmentsPerCore,filtersPerCore))\n",
    "# x could be rounded here (floor) allowing non square y. Example 2 locations per core, x=1 y=2 instead of\n",
    "# x = y = floor(sqrt(2)) = 1\n",
    "filterExtentPerCore_y = np.divide(numCompartmentsPerCore,filtersPerCore)/filterExtentPerCore_x\n",
    "\n",
    "# weights per core is a measure of core memory needed to store weights\n",
    "# core splitting (numCoreSplits) and grouping (numGroups) reduce weightsPerCore\n",
    "weightsPerCore = filtersPerCore*fanIn\n",
    "\n",
    "# how many synapses on each core\n",
    "# core splitting (numCoreSplits) and grouping (numGroups) reduce synapsesPerCore\n",
    "synapsesPerCore = filtersPerCore*fanIn*filterExtentPerCore_x*filterExtentPerCore_y\n",
    "\n",
    "# Everything else revolves around this number which we hardcoded above, but if we choose to fix a different number\n",
    "# in the future (N3 configurable, or something else is the bottleneck) this is how it would be calculated from\n",
    "# other parameters\n",
    "compartmentsPerCore = filtersPerCore*filterExtentPerCore_x*filterExtentPerCore_y\n",
    "\n",
    "# One axon routes to synapses for all filters and all x-y within extent (or core, depending on which is small)\n",
    "# assumes there is not gap in input (stride<=filter_x) and (stride<=filter_y)\n",
    "assert (stride<=filter_x).all and (stride<=filter_y).all, \"stride must be less than filter size\"\n",
    "axonsPerCore = np.divide(input_c,numGroups)*(filterExtentPerCore_x*stride-1+filter_x)*(filterExtentPerCore_y*stride-1+filter_y)\n",
    "\n",
    "# How many cores and chips for this layer\n",
    "numCores = numNeurons/compartmentsPerCore\n",
    "numChips = numCores/numCoresPerChip\n",
    "\n",
    "# How many cores does each input spike need to route to?\n",
    "numCoresPerSpike = (np.divide(filter_x,stride)/filterExtentPerCore_x)*(np.divide(filter_y,stride)/filterExtentPerCore_y)/numCoreSplits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  #  |     Input Size   |  Filter Size \t|Str Grp Spl|\t  Weights    |  Compartments  |    Synapses    |Axons\t|cor/spk|core\n",
      "  0  |224 x 224 x    3\t| 3 x  3 x    3\t| 2   1   1 |8.6e+02(8.6e+02)|4.0e+05(1.0e+03)|1.1e+07(2.8e+04)|(532)\t|0.07\t|392\n",
      "  1  |112 x 112 x   32\t| 1 x  1 x   32\t| 1   1   1 |1.0e+03(1.0e+03)|4.0e+05(1.0e+03)|1.3e+07(3.3e+04)|(1024)\t|0.03\t|392\n",
      "  2  |112 x 112 x   32\t| 3 x  3 x    1\t| 1   1   1 |2.9e+02(2.9e+02)|4.0e+05(1.0e+03)|3.6e+06(9.2e+03)|(1876)\t|0.28\t|392\n",
      "  3  |112 x 112 x   32\t| 1 x  1 x   32\t| 1   1   1 |5.1e+02(5.1e+02)|2.0e+05(1.0e+03)|6.4e+06(3.3e+04)|(2048)\t|0.02\t|196\n",
      "  4  |112 x 112 x   16\t| 1 x  1 x   16\t| 1   1   1 |1.5e+03(1.5e+03)|1.2e+06(1.0e+03)|1.9e+07(1.6e+04)|(171)\t|0.09\t|1176\n",
      "  5  |112 x 112 x   96\t| 3 x  3 x    1\t| 2   1   1 |8.6e+02(8.6e+02)|3.0e+05(1.0e+03)|2.7e+06(9.2e+03)|(6988)\t|0.21\t|294\n",
      "  6  | 56 x  56 x   96\t| 1 x  1 x   96\t| 1   1   1 |2.3e+03(2.3e+03)|7.5e+04(1.0e+03)|7.2e+06(9.8e+04)|(4096)\t|0.02\t|74\n",
      "  7  | 56 x  56 x   24\t| 1 x  1 x   24\t| 1   1   1 |3.5e+03(3.5e+03)|4.5e+05(1.0e+03)|1.1e+07(2.5e+04)|(171)\t|0.14\t|441\n",
      "  8  | 56 x  56 x  144\t| 3 x  3 x    1\t| 2   1   1 |1.3e+03(1.3e+03)|1.1e+05(1.0e+03)|1.0e+06(9.2e+03)|(7744)\t|0.32\t|110\n",
      "  9  | 28 x  28 x  144\t| 1 x  1 x  144\t| 1   1   1 |3.5e+03(3.5e+03)|1.9e+04(1.0e+03)|2.7e+06(1.5e+05)|(6144)\t|0.02\t|18\n",
      " 10  | 56 x  56 x   24\t| 1 x  1 x   24\t| 1   1   1 |3.5e+03(3.5e+03)|4.5e+05(1.0e+03)|1.1e+07(2.5e+04)|(171)\t|0.14\t|441\n",
      " 11  | 56 x  56 x  144\t| 3 x  3 x    1\t| 2   1   1 |1.3e+03(1.3e+03)|1.1e+05(1.0e+03)|1.0e+06(9.2e+03)|(7744)\t|0.32\t|110\n",
      " 12  | 28 x  28 x  144\t| 1 x  1 x  144\t| 1   1   1 |4.6e+03(4.6e+03)|2.5e+04(1.0e+03)|3.6e+06(1.5e+05)|(4608)\t|0.03\t|24\n",
      " 13  | 28 x  28 x   32\t| 1 x  1 x   32\t| 1   1   1 |6.1e+03(6.1e+03)|1.5e+05(1.0e+03)|4.8e+06(3.3e+04)|(171)\t|0.19\t|147\n",
      " 14  | 28 x  28 x  192\t| 3 x  3 x    1\t| 2   1   1 |1.7e+03(1.7e+03)|3.8e+04(1.0e+03)|3.4e+05(9.2e+03)|(8411)\t|0.42\t|37\n",
      " 15  | 14 x  14 x  192\t| 1 x  1 x  192\t| 1   1   1 |6.1e+03(6.1e+03)|6.3e+03(1.0e+03)|1.2e+06(2.0e+05)|(6144)\t|0.03\t|6\n",
      " 16  | 28 x  28 x   32\t| 1 x  1 x   32\t| 1   1   1 |6.1e+03(6.1e+03)|1.5e+05(1.0e+03)|4.8e+06(3.3e+04)|(171)\t|0.19\t|147\n",
      " 17  | 28 x  28 x  192\t| 3 x  3 x    1\t| 2   1   1 |1.7e+03(1.7e+03)|3.8e+04(1.0e+03)|3.4e+05(9.2e+03)|(8411)\t|0.42\t|37\n",
      " 18  | 14 x  14 x  192\t| 1 x  1 x  192\t| 1   1   1 |6.1e+03(6.1e+03)|6.3e+03(1.0e+03)|1.2e+06(2.0e+05)|(6144)\t|0.03\t|6\n",
      " 19  | 28 x  28 x   32\t| 1 x  1 x   32\t| 1   1   1 |6.1e+03(6.1e+03)|1.5e+05(1.0e+03)|4.8e+06(3.3e+04)|(171)\t|0.19\t|147\n",
      " 20  | 28 x  28 x  192\t| 3 x  3 x    1\t| 2   1   1 |1.7e+03(1.7e+03)|3.8e+04(1.0e+03)|3.4e+05(9.2e+03)|(8411)\t|0.42\t|37\n",
      " 21  | 14 x  14 x  192\t| 1 x  1 x  192\t| 1   1   1 |1.2e+04(1.2e+04)|1.3e+04(1.0e+03)|2.4e+06(2.0e+05)|(3072)\t|0.06\t|12\n",
      " 22  | 14 x  14 x   64\t| 1 x  1 x   64\t| 1   1   1 |2.5e+04(2.5e+04)|7.5e+04(1.0e+03)|4.8e+06(6.6e+04)|(171)\t|0.37\t|74\n",
      " 23  | 14 x  14 x  384\t| 3 x  3 x    1\t| 2   1   1 |3.5e+03(3.5e+03)|1.9e+04(1.0e+03)|1.7e+05(9.2e+03)|(10649)\t|0.84\t|18\n",
      " 24  |  7 x   7 x  384\t| 1 x  1 x  384\t| 1   1   1 |2.5e+04(2.5e+04)|3.1e+03(1.0e+03)|1.2e+06(3.9e+05)|(6144)\t|0.06\t|3\n",
      " 25  | 14 x  14 x   64\t| 1 x  1 x   64\t| 1   1   1 |2.5e+04(2.5e+04)|7.5e+04(1.0e+03)|4.8e+06(6.6e+04)|(171)\t|0.37\t|74\n",
      " 26  | 14 x  14 x  384\t| 3 x  3 x    1\t| 2   1   1 |3.5e+03(3.5e+03)|1.9e+04(1.0e+03)|1.7e+05(9.2e+03)|(10649)\t|0.84\t|18\n",
      " 27  |  7 x   7 x  384\t| 1 x  1 x  384\t| 1   1   1 |2.5e+04(2.5e+04)|3.1e+03(1.0e+03)|1.2e+06(3.9e+05)|(6144)\t|0.06\t|3\n",
      " 28  | 14 x  14 x   64\t| 1 x  1 x   64\t| 1   1   1 |2.5e+04(2.5e+04)|7.5e+04(1.0e+03)|4.8e+06(6.6e+04)|(171)\t|0.37\t|74\n",
      " 29  | 14 x  14 x  384\t| 3 x  3 x    1\t| 2   1   1 |3.5e+03(3.5e+03)|1.9e+04(1.0e+03)|1.7e+05(9.2e+03)|(10649)\t|0.84\t|18\n",
      " 30  |  7 x   7 x  384\t| 1 x  1 x  384\t| 1   1   1 |2.5e+04(2.5e+04)|3.1e+03(1.0e+03)|1.2e+06(3.9e+05)|(6144)\t|0.06\t|3\n",
      " 31  | 14 x  14 x   64\t| 1 x  1 x   64\t| 1   1   1 |2.5e+04(2.5e+04)|7.5e+04(1.0e+03)|4.8e+06(6.6e+04)|(171)\t|0.37\t|74\n",
      " 32  | 14 x  14 x  384\t| 3 x  3 x    1\t| 1   1   1 |3.5e+03(3.5e+03)|7.5e+04(1.0e+03)|6.8e+05(9.2e+03)|(5068)\t|3.38\t|74\n",
      " 33  | 14 x  14 x  384\t| 1 x  1 x  384\t| 1   1   1 |3.7e+04(3.7e+04)|1.9e+04(1.0e+03)|7.2e+06(3.9e+05)|(4096)\t|0.09\t|18\n",
      " 34  | 14 x  14 x   96\t| 1 x  1 x   96\t| 1   1   1 |5.5e+04(5.5e+04)|1.1e+05(1.0e+03)|1.1e+07(9.8e+04)|(171)\t|0.56\t|110\n",
      " 35  | 14 x  14 x  576\t| 3 x  3 x    1\t| 1   1   1 |5.2e+03(5.2e+03)|1.1e+05(1.0e+03)|1.0e+06(9.2e+03)|(6400)\t|5.06\t|110\n",
      " 36  | 14 x  14 x  576\t| 1 x  1 x  576\t| 1   1   1 |5.5e+04(5.5e+04)|1.9e+04(1.0e+03)|1.1e+07(5.9e+05)|(6144)\t|0.09\t|18\n",
      " 37  | 14 x  14 x   96\t| 1 x  1 x   96\t| 1   1   1 |5.5e+04(5.5e+04)|1.1e+05(1.0e+03)|1.1e+07(9.8e+04)|(171)\t|0.56\t|110\n",
      " 38  | 14 x  14 x  576\t| 3 x  3 x    1\t| 1   1   1 |5.2e+03(5.2e+03)|1.1e+05(1.0e+03)|1.0e+06(9.2e+03)|(6400)\t|5.06\t|110\n",
      " 39  | 14 x  14 x  576\t| 1 x  1 x  576\t| 1   1   1 |5.5e+04(5.5e+04)|1.9e+04(1.0e+03)|1.1e+07(5.9e+05)|(6144)\t|0.09\t|18\n",
      " 40  | 14 x  14 x   96\t| 1 x  1 x   96\t| 1   1   1 |5.5e+04(5.5e+04)|1.1e+05(1.0e+03)|1.1e+07(9.8e+04)|(171)\t|0.56\t|110\n",
      " 41  | 14 x  14 x  576\t| 3 x  3 x    1\t| 2   1   1 |5.2e+03(5.2e+03)|2.8e+04(1.0e+03)|2.5e+05(9.2e+03)|(12544)\t|1.27\t|28\n",
      " 42  |  7 x   7 x  576\t| 1 x  1 x  576\t| 1   1   1 |9.2e+04(9.2e+04)|7.8e+03(1.0e+03)|4.5e+06(5.9e+05)|(3686)\t|0.16\t|8\n",
      " 43  |  7 x   7 x  160\t| 1 x  1 x  160\t| 1   1   1 |1.5e+05(1.5e+05)|4.7e+04(1.0e+03)|7.5e+06(1.6e+05)|(171)\t|0.94\t|46\n",
      " 44  |  7 x   7 x  960\t| 3 x  3 x    1\t| 2   1   1 |8.6e+03(8.6e+03)|1.2e+04(1.0e+03)|1.1e+05(9.2e+03)|(15868)\t|2.11\t|11\n",
      " 45  |  4 x   4 x  960\t| 1 x  1 x  960\t| 1   1   1 |1.5e+05(1.5e+05)|2.0e+03(1.0e+03)|1.9e+06(9.8e+05)|(6144)\t|0.16\t|2\n",
      " 46  |  7 x   7 x  160\t| 1 x  1 x  160\t| 1   1   1 |1.5e+05(1.5e+05)|4.7e+04(1.0e+03)|7.5e+06(1.6e+05)|(171)\t|0.94\t|46\n",
      " 47  |  7 x   7 x  960\t| 3 x  3 x    1\t| 2   1   1 |8.6e+03(8.6e+03)|1.2e+04(1.0e+03)|1.1e+05(9.2e+03)|(15868)\t|2.11\t|11\n",
      " 48  |  4 x   4 x  960\t| 1 x  1 x  960\t| 1   1   1 |1.5e+05(1.5e+05)|2.0e+03(1.0e+03)|1.9e+06(9.8e+05)|(6144)\t|0.16\t|2\n",
      " 49  |  7 x   7 x  160\t| 1 x  1 x  160\t| 1   1   1 |1.5e+05(1.5e+05)|4.7e+04(1.0e+03)|7.5e+06(1.6e+05)|(171)\t|0.94\t|46\n",
      " 50  |  7 x   7 x  960\t| 3 x  3 x    1\t| 1   1   1 |8.6e+03(8.6e+03)|4.7e+04(1.0e+03)|4.2e+05(9.2e+03)|(8830)\t|8.44\t|46\n",
      " 51  |  7 x   7 x  960\t| 1 x  1 x  960\t| 1   1   1 |3.1e+05(3.1e+05)|1.6e+04(1.0e+03)|1.5e+07(9.8e+05)|(3072)\t|0.31\t|15\n",
      " 52  |  7 x   7 x  320\t| 1 x  1 x  320\t| 1   1   1 |4.1e+05(4.1e+05)|6.3e+04(1.0e+03)|2.0e+07(3.3e+05)|(256)\t|1.25\t|61\n",
      "     |\t\t\t|\t\t|\t    |2.2e+06\t     |6.1e+06\t      |2.7e+08\t       |\t|\t|5997\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=2, suppress=True)\n",
    "print(\\\n",
    "    \"  #  |\"\\\n",
    "      + \"     Input Size   |\"\\\n",
    "      + \"  Filter Size \\t|\"\\\n",
    "      + \"Str Grp Spl|\"\\\n",
    "      + \"\\t  Weights    |\"\\\n",
    "      + \"  Compartments  |\"\\\n",
    "      + \"    Synapses    |\"\\\n",
    "      + \"Axons\\t|\"\\\n",
    "      + \"cor/spk|\"\\\n",
    "      + \"core\"\\\n",
    "     )\n",
    "for ii in range(len(numFilters)):\n",
    "    print(\\\n",
    "          \"{:3d}  \"                 #\"|Layer number|\"\\\n",
    "          \"|{:3.0f} x {:3.0f} x {:4.0f}\\t\"         #\"Input Size|\"\\\n",
    "          \"|{:2.0f} x {:2.0f} x {:4.0f}\\t\"         #\"Filter Size|\"\\\n",
    "          \"|{:2.0f} {:3.0f} {:3.0f} \"                 #\"|Stride|\"\\\n",
    "          \"|{:1.1e}({:1.1e})\"           #\"|Weights|\"\\\n",
    "          \"|{:1.1e}({:1.1e})\"   #\"|Compartments|\"\\\n",
    "          \"|{:1.1e}({:1.1e})\"             #\"|Synapses|\"\\\n",
    "          \"|({:.0f})\\t\"               #\"|Axons|\"\\\n",
    "          \"|{:.2f}\\t\"                 #\"|Cores/spike|\"\\\n",
    "          \"|{:.0f}\"                 #\"|Cores|\"\\\n",
    "          .format( \\\n",
    "          ii,                      #\"|Layer number|\"\\\n",
    "          input_w[ii], input_h[ii], input_c[ii],#\"Input Size|\"\\\n",
    "          filter_x[ii], filter_y[ii], filter_c[ii],#\"Filter Size|\"\\\n",
    "          stride[ii],              #\"|Stride|\"\\\n",
    "          numGroups[ii],           #\"|Groups|\"\\\n",
    "          numCoreSplits[ii],       #\"|Core Split|\"\\\n",
    "          numWeights[ii], weightsPerCore[ii],#\"|Weights|\"\\\n",
    "          numNeurons[ii], compartmentsPerCore[ii],#\"|Compartments|\"\\\n",
    "          numSynapses[ii], synapsesPerCore[ii],#\"|Synapses|\"\\\n",
    "          axonsPerCore[ii],        #\"|Axons|\"\\\n",
    "          numCoresPerSpike[ii],    #\"|Cores/spike|\"\\\n",
    "          numCores[ii]                 #\"|Cores|\"\\\n",
    "         ))\n",
    "    \n",
    "print(\\\n",
    "      \"     \"                 #\"|Layer number|\"\\\n",
    "      \"|\\t\\t\\t\"         #\"Input Size|\"\\\n",
    "      \"|\\t\\t\"         #\"Filter Size|\"\\\n",
    "      \"|\\t    \"                 #\"|Stride|\"\\\n",
    "      \"|{:1.1e}\\t     \"           #\"|Weights|\"\\\n",
    "      \"|{:1.1e}\\t      \"   #\"|Compartments|\"\\\n",
    "      \"|{:1.1e}\\t       \"             #\"|Synapses|\"\\\n",
    "      \"|\\t\"               #\"|Axons|\"\\\n",
    "      \"|\\t\"                 #\"|Cores/spike|\"\\\n",
    "      \"|{:.0f}\"                 #\"|Cores|\"\\\n",
    "      .format( \\\n",
    "              np.sum(numWeights),#\"|Weights|\"\\\n",
    "              np.sum(numNeurons),#\"|Compartments|\"\\\n",
    "              np.sum(numSynapses),#\"|Synapses|\"\\\n",
    "              np.sum(numCores)                 #\"|Cores|\"\\\n",
    "             ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
